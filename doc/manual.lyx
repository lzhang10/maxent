#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\usepackage{html}
\usepackage{listings}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage{doxygen}
\end_preamble
\options  openany
\use_default_options false
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize letterpaper
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\series bold
Maximum Entropy Modeling Toolkit 
\begin_inset Newline newline
\end_inset

for Python and C++
\end_layout

\begin_layout Author
Le Zhang
\begin_inset Newline newline
\end_inset

github.com/lzhang10
\begin_inset Newline newline
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
pagebreak
\end_layout

\end_inset


\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
What is it
\end_layout

\begin_layout Standard
This package provides a Maximum Entropy Modeling toolkit written in C++
 with Python binding.
 In particular, it includes: 
\end_layout

\begin_layout Itemize
Conditional Maximum Entropy Model 
\end_layout

\begin_layout Itemize
L-BFGS Parameter Estimation 
\end_layout

\begin_layout Itemize
GIS Parameter Estimation 
\end_layout

\begin_layout Itemize
Gaussian Prior Smoothing 
\end_layout

\begin_layout Itemize
C++ API 
\end_layout

\begin_layout Itemize
Python3 Extension module
\end_layout

\begin_layout Standard
If you do not know what Maximum Entropy Model (MaxEnt) is, please refer
 to chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:intro"

\end_inset

 for a brief introduction or section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:reading"

\end_inset

 for some recommended papers.
\end_layout

\begin_layout Section
License
\end_layout

\begin_layout Standard
This software grew out of an attempt to port the Java maxent package (http://max
ent.sourceforge.net) into C++ in the early 2000s.
 Therefore it follows the same license used by the java maxent package.
 The library is freeware and is licensed under the LGPL license (see LICENSE
 file for more detail, or visit 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://www.gnu.org/copyleft/lesser.html
\end_layout

\end_inset

).
 It is distributed with full source code and documentation.
\end_layout

\begin_layout Section
Known Problem
\end_layout

\begin_layout Standard
Sometimes the L-BFGS training can stop with an error if you use a small
 Gaussian prior value (-g).
 It seems this problem only occurs on small training data.
\end_layout

\begin_layout Chapter
Introduction to Maximum Entropy Modeling
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chap:intro"

\end_inset

 This section provides a brief introduction to Maximum Entropy Modeling.
 It is by no means complete, the reader is referred to 
\begin_inset Quotes eld
\end_inset

further reading
\begin_inset Quotes erd
\end_inset

 at the end of this section to learn more about this subject.
\end_layout

\begin_layout Standard
Maximum Entropy (ME, or maxent for short) model is a general purpose machine
 learning framework that has been successfully applied in various fields
 including spatial physics, computer vision, and Natural Language Processing
 (NLP).
 This introduction will focus on the application of maxent model to NLP
 tasks.
 However, it is straightforward to extend the technique described here to
 other domains.
\end_layout

\begin_layout Section
The Modeling Problem
\end_layout

\begin_layout Standard
The goal of statistical modeling is to construct a model that best accounts
 for some training data.
 More specifically, for a given empirical probability distribution 
\begin_inset Formula $\tilde{p}$
\end_inset

, we want to build a model 
\begin_inset Formula $p$
\end_inset

 as close to 
\begin_inset Formula $\tilde{p}$
\end_inset

 as possible.
\end_layout

\begin_layout Standard
Of course, given a set of training data, there are numerous ways to choose
 a model 
\begin_inset Formula $p$
\end_inset

 that accounts for the data.
 It can be shown that the probability distribution of the form 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:me"

\end_inset

 is the one that is closest to 
\begin_inset Formula $\tilde{p}$
\end_inset

 in the sense of Kullback-Leibler divergence, when subjected to a set of
 feature constraints: 
\begin_inset Formula 
\begin{equation}
p(y\mid x)=\frac{1}{Z(x)}\exp\left[\sum_{i=1}^{k}\lambda_{i}f_{i}(x,y)\right]\label{eq:me}
\end{equation}

\end_inset

 here 
\begin_inset Formula $p(y\mid x)$
\end_inset

 denotes the conditional probability of predicting an 
\emph on
outcome
\emph default
 
\begin_inset Formula $y$
\end_inset

 on seeing the 
\emph on
context
\emph default
 
\begin_inset Formula $x$
\end_inset

.
 
\begin_inset Formula $f_{i}(x,y)'s$
\end_inset

 are feature functions (described in detail later), 
\begin_inset Formula $\lambda_{i}'s$
\end_inset

 are the weighting parameters for 
\begin_inset Formula $f_{i}(x,y)'s$
\end_inset

.
 
\begin_inset Formula $k$
\end_inset

 is the number of features and 
\begin_inset Formula $Z(x)$
\end_inset

 is a normalization factor (often called partition function) to ensure that
 
\begin_inset Formula $\sum_{y}p(y|x)=1$
\end_inset

.
\end_layout

\begin_layout Standard
ME model represents evidence with binary functions
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Actually, ME model can have non-negative integer or real feature values.
 We restrict our discussion to binary value feature here, which is the most
 common feature type used in NLP.
 This toolkit fully supports real feature values.
\end_layout

\end_inset

 known as 
\emph on
contextual predicates
\emph default
 in the form: 
\begin_inset Formula 
\begin{equation}
f_{cp,y'}(x,y)=\left\{ \begin{array}{ll}
1 & \mbox{if }y=y'\mbox{ and }cp(x)=true\\
0 & \mbox{otherwise}
\end{array}\right.
\end{equation}

\end_inset

 where 
\begin_inset Formula $cp$
\end_inset

 is the 
\emph on
contextual predicate
\emph default
 that maps a pair of 
\emph on
outcome
\emph default
 
\begin_inset Formula $y$
\end_inset

 and 
\emph on
context
\emph default
 
\begin_inset Formula $x$
\end_inset

 to 
\begin_inset Formula $\{true,false\}$
\end_inset

.
\end_layout

\begin_layout Standard
The modeler can choose arbitrary feature functions in order to reflect the
 characteristic of the problem domain as faithfully as possible.
 The ability of freely incorporating various problem-specific knowledge
 in terms of feature functions gives ME models the obvious advantage over
 other learn paradigms, which often suffer from strong feature independence
 assumption (such as naive bayes classifier).
\end_layout

\begin_layout Standard
For instance, in part-of-speech tagging, a process that assigns part-of-speech
 tags to words in a sentence, a useful feature may be: 
\begin_inset Formula 
\[
f_{previous\_tag\_is\_DETERMINER,NOUN}(x,y)=\left\{ \begin{array}{ll}
1 & \mbox{if }y=NOUN\mbox{ and }previous\_tag\_is\_DETERMINER(x)=true\\
0 & \mbox{otherwise}
\end{array}\right.
\]

\end_inset

 which is 
\emph on
activated
\emph default
 when previous tag is DETERMINER and current word's tag is NOUN.
\end_layout

\begin_layout Standard
In Text Categorization task, a feature may look like: 
\begin_inset Formula 
\[
f_{document\_has\_ROMANTIC,love\_story}(x,y)=\left\{ \begin{array}{ll}
1 & \mbox{if }y=love_{s}tory\mbox{ and }document\_contains\_ROMANTIC(x)=true\\
0 & \mbox{otherwise}
\end{array}\right.
\]

\end_inset

 which is 
\emph on
activated
\emph default
 when the term ROMANTIC is found in a document labeled as type:love_story.
\end_layout

\begin_layout Standard
Once a set of features is chosen by the modeler, we can construct the correspond
ing maxent model by adding features as constraints to the model and adjust
 weights of these features.
 Formally, We require that: 
\begin_inset Formula 
\[
E_{\tilde{p}}<f_{i}>=E_{p}<f_{i}>
\]

\end_inset

 Where 
\begin_inset Formula $E_{\tilde{p}}<f_{i}>=\sum_{x}\tilde{p}(x,y)f_{i}(x,y)$
\end_inset

 is the empirical expectation of feature 
\begin_inset Formula $f_{i}(x,y)$
\end_inset

 in the training data and 
\begin_inset Formula $E_{p}<f_{i}>=\sum_{x}p(x,y)f_{i}(x,y)$
\end_inset

 is the feature expectation with respect to the model distribution 
\begin_inset Formula $p$
\end_inset

.
 Among all the models subjected to these constraints there is one with the
 Maximum Entropy, usually called the 
\emph on
Maximum Entropy Solution
\emph default
.
 
\end_layout

\begin_layout Section
Parameter Estimation
\end_layout

\begin_layout Standard
Given an exponential model with 
\begin_inset Formula $n$
\end_inset

 features and a set of training data (empirical distribution), we need to
 find the associated real-value weights for each of the 
\begin_inset Formula $n$
\end_inset

 features which maximize the model's log-likelihood: 
\begin_inset Formula 
\begin{equation}
L(p)=\sum_{x,y}\tilde{p}(x,y)\log p(y\mid x)\label{eq:pseudo_loglikelihood}
\end{equation}

\end_inset

 Selecting an optimal model subjected to given contains from the exponential
 (log-linear) family is not a trivial task.
 There are two popular iterative scaling algorithms specially designed to
 estimate parameters of ME models of the form 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:me"

\end_inset

: Generalized Iterative Scaling 
\begin_inset CommandInset citation
LatexCommand citep
key "darroch1972annals"
literal "true"

\end_inset

 and Improved Iterative Scaling 
\begin_inset CommandInset citation
LatexCommand citep
key "dellapietra97inducing"
literal "true"

\end_inset

.
\end_layout

\begin_layout Standard
Recently, another general purpose optimize method 
\emph on
Limited-Memory Variable Metric
\emph default
 (L-BFGS for short) method has been found to be especially effective for
 maxent parameters estimating problem 
\begin_inset CommandInset citation
LatexCommand citep
key "malouf-comparison"
literal "true"

\end_inset

.
 L-BFGS is the default parameter estimating method used by this toolkit.
 
\end_layout

\begin_layout Section
Further Reading
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "sec:reading"

\end_inset

 This section lists some recommended papers for your further reference.
 
\end_layout

\begin_layout Itemize
Maximum Entropy Approach to Natural Language Processing 
\begin_inset CommandInset citation
LatexCommand citep
key "berger96maximum"
literal "true"

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
A must read paper on applying maxent technique to Natural Language Processing.
 This paper describes maxent in detail and presents an Increment Feature
 Selection algorithm for increasingly construct a maxent model as well as
 several example in statistical Machine Translation.
 
\end_layout

\end_deeper
\begin_layout Itemize
Inducing Features of Random Fields 
\begin_inset CommandInset citation
LatexCommand citep
key "dellapietra97inducing"
literal "true"

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Another must read paper on maxent.
 It deals with a more general frame work: 
\emph on
Random Fields
\emph default
 and proposes an 
\emph on
Improved Iterative Scaling
\emph default
 algorithm for estimating parameters of Random Fields.
 This paper gives theoretical background to Random Fields (and hence Maxent
 model).
 A greedy 
\emph on
Field Induction
\emph default
 method is presented to automatically construct a detail random fields from
 a set of atomic features.
 An word morphology application for English is developed.
\end_layout

\end_deeper
\begin_layout Itemize
Adaptive Statistical Language Modeling: A Maximum Entropy Approach 
\begin_inset CommandInset citation
LatexCommand citep
key "rosenfeld96maximum"
literal "true"

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
This paper applied ME technique to statistical language modeling task.
 More specifically, it built a conditional Maximum Entropy model that incorporat
ed traditional N-gram, distant N-gram and trigger pair features.
 Significantly perplexity reduction over baseline trigram model was reported.
 Later, Rosenfeld and his group proposed a 
\emph on
Whole Sentence Exponential Model
\emph default
 that overcome the computation bottleneck of conditional ME model.
 
\end_layout

\end_deeper
\begin_layout Itemize
Maximum Entropy Models For Natural Language Ambiguity Resolution 
\begin_inset CommandInset citation
LatexCommand citep
key "ratnaparkhi98maximum"
literal "true"

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
This dissertation discussed the application of maxent model to various Natural
 Language Dis-ambiguity tasks in detail.
 Several problems were attacked within the ME framework: sentence boundary
 detection, part-of-speech tagging, shallow parsing and text categorization.
 Comparison with other machine learning technique (Naive Bayes, Transform
 Based Learning, Decision Tree etc.) are given.
 
\end_layout

\end_deeper
\begin_layout Itemize
The Improved Iterative Scaling Algorithm: A Gentle Introduction 
\begin_inset CommandInset citation
LatexCommand citep
key "berger97improved"
literal "true"

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
This paper describes IIS algorithm in detail.
 The description is easier to understand than 
\begin_inset CommandInset citation
LatexCommand citep
key "dellapietra97inducing"
literal "true"

\end_inset

, which involves more mathematical notations.
 
\end_layout

\end_deeper
\begin_layout Itemize
Stochastic Attribute-Value Grammars (Abney, 1997)
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Abney applied Improved Iterative Scaling algorithm to parameters estimation
 of Attribute-Value grammars, which can not be corrected calculated by ERF
 method (though it works on PCFG).
 Random Fields is the model of choice here with a general Metropolis-Hasting
 Sampling on calculating feature expectation under newly constructed model.
 
\end_layout

\end_deeper
\begin_layout Itemize
A comparison of algorithms for maximum entropy parameter estimation 
\begin_inset CommandInset citation
LatexCommand citep
key "malouf-comparison"
literal "true"

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Four iterative parameter estimation algorithms were compared on several
 NLP tasks.
 L-BFGS was observed to be the most effective parameter estimation method
 for Maximum Entropy model, much better than IIS and GIS.
 
\begin_inset CommandInset citation
LatexCommand citep
key "wallach-efficient"
literal "true"

\end_inset

 reported similar results on parameter estimation of Conditional Random
 Fields.
 
\end_layout

\end_deeper
\begin_layout Chapter
Tutorial
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chap:tutorial"

\end_inset

 The purpose of this section is twofold: first, it covers the basic steps
 required to build and use a Conditional Maximum Entropy Model with this
 toolkit.
 Second, it demonstrates the powerfulness of maxent modeling technique by
 building an English part-of-speech tagger with the Python 
\emph on
maxent
\emph default
 extension.
\end_layout

\begin_layout Section
Representing Features
\end_layout

\begin_layout Standard
Follow the description of 
\begin_inset CommandInset citation
LatexCommand citep
key "ratnaparkhi98maximum"
literal "true"

\end_inset

, the mathematical representation of a feature used in a Conditional Maximum
 Entropy Model can be written as: 
\begin_inset Formula 
\begin{equation}
f_{cp,y'}(x,y)=\left\{ \begin{array}{ll}
1 & \mbox{if }y=y'\mbox{ and }cp(x)=true\\
0 & \mbox{otherwise}
\end{array}\right.\label{eq:me_feature}
\end{equation}

\end_inset

 where 
\begin_inset Formula $cp$
\end_inset

 is the 
\emph on
contextual predicate
\emph default
 which maps a pair of 
\emph on
outcome
\emph default
 
\begin_inset Formula $y$
\end_inset

 and 
\emph on
context
\emph default
 
\begin_inset Formula $x$
\end_inset

 into 
\begin_inset Formula $\{true,false\}$
\end_inset

.
\end_layout

\begin_layout Standard
This kind of math notation must be expressed as features of literal string
 in order to be used in this toolkit.
 So a feature in part-of-speech tagger which has the form: 
\begin_inset Formula 
\begin{equation}
f_{previous\_tag\_is\_DETERMINER,NOUN}(x,y)=\left\{ \begin{array}{ll}
1 & \mbox{if }y=NOUN\mbox{ and }previous\_tag\_is\_DETERMINER(x)=true\\
0 & \mbox{otherwise}
\end{array}\right.
\end{equation}

\end_inset

 can be written as a literal string: 
\begin_inset Quotes eld
\end_inset

tag-1=DETERMINER_NOUN
\begin_inset Quotes erd
\end_inset

.
 You will see more concrete examples in the Case Study section.
\end_layout

\begin_layout Section
Create a Maxent Model Instance
\end_layout

\begin_layout Standard
A 
\emph on
maxent
\emph default
 instance can be created by calling its constructor: 
\begin_inset Newline newline
\end_inset

 In C++: 
\end_layout

\begin_layout LyX-Code
#include <maxent/maxentmodel.hpp>
\begin_inset Newline newline
\end_inset

using namespace maxent;
\begin_inset Newline newline
\end_inset

MaxentModel m;
\end_layout

\begin_layout Standard
This will create an instance of MaxentModel class called 
\emph on
m
\emph default
.
 Please note that all classes and functions are in the namespace 
\emph on
maxent
\emph default
.
 For illustration purpose, the include and using statements will be ignored
 intentionally for the rest of this tutorial.
\end_layout

\begin_layout Standard
In Python:
\end_layout

\begin_layout LyX-Code
from maxent import MaxentModel
\begin_inset Newline newline
\end_inset

m = MaxentModel()
\end_layout

\begin_layout Standard
The first statement 
\emph on
import
\emph default
 imports our main class MaxentModel from 
\emph on
maxent
\emph default
 module into current scope.
 The second statement creates an instance of MaxentModel class.
 
\end_layout

\begin_layout Section
Adding Events to Model
\end_layout

\begin_layout Standard
Typically, training data consists of a set of events (samples).
 Each event has a 
\emph on
context
\emph default
, an 
\emph on
outcome
\emph default
, and a 
\emph on
count
\emph default
 indicating how many times this event occurs in training data.
\end_layout

\begin_layout Standard
Remember that a 
\emph on
context
\emph default
 is just a group of 
\emph on
context predicates
\emph default
.
 Thus an event will have the form: 
\begin_inset Formula 
\[
[(predicate_{1},predicate_{2},\dots,predicate_{n}),outcome,count]
\]

\end_inset


\end_layout

\begin_layout Standard
Suppose we want to add the following event to our model: 
\begin_inset Formula 
\[
[(predicate_{1},predicate_{2},predicate_{3}),outcome1,1]
\]

\end_inset


\end_layout

\begin_layout Standard
We need to first create a 
\emph on
context
\emph default
:
\begin_inset Foot
status open

\begin_layout Plain Layout
It's possible to specify feature value when creating a context: 
\end_layout

\begin_layout Plain Layout
In C++: 
\end_layout

\begin_layout LyX-Code
std::vector<pair<std::string, float> > context
\begin_inset Newline newline
\end_inset

context.append(make_pair(``predicate1'', 2.0));
\begin_inset Newline newline
\end_inset

context.append(make_pair(``predicate2'', 3.0));
\begin_inset Newline newline
\end_inset

context.append(make_pair(``predicate3'', 4.0));
\begin_inset Newline newline
\end_inset

.
 .
 .
\end_layout

\begin_layout Plain Layout
This is simpler in Python: 
\end_layout

\begin_layout LyX-Code
context = [('predicate1', 2.0), ('predicate2', 3.0), ('predicate3', 4.0)]
\end_layout

\begin_layout Plain Layout
For illustration purpose, we will only cover binary cases (which is more
 common).
 You can find more information on specifying real feature value in the API
 section.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In C++: 
\end_layout

\begin_layout LyX-Code
std::vector<std::string> context
\begin_inset Newline newline
\end_inset

context.append(``predicate1'');
\begin_inset Newline newline
\end_inset

context.append(``predicate2'');
\begin_inset Newline newline
\end_inset

context.append(``predicate3'');
\begin_inset Newline newline
\end_inset

.
 .
 .
\end_layout

\begin_layout Standard
In Python: 
\end_layout

\begin_layout LyX-Code
context = ['predicate1', 'predicate2', 'predicate3']
\end_layout

\begin_layout Standard
Before any event can be added, one must call 
\emph on
begin_add_event()
\emph default
 to inform the model the beginning of training.
\end_layout

\begin_layout Standard
In C++:
\end_layout

\begin_layout LyX-Code
m.begin_add_event();
\end_layout

\begin_layout Standard
In Python: 
\end_layout

\begin_layout LyX-Code
m.begin_add_event()
\end_layout

\begin_layout Standard
Now we are ready to add events: In C++:
\end_layout

\begin_layout LyX-Code
m.add_event(context, "outcome1", 1);
\end_layout

\begin_layout Standard
In Python:
\end_layout

\begin_layout LyX-Code
m.add_event(context, "outcome1", 1)
\end_layout

\begin_layout Standard
The third argument of 
\emph on
add_event()
\emph default
 is the count of the event and can be ignored if the count is 1.
\end_layout

\begin_layout Standard
One can repeatedly call 
\emph on
add_event()
\emph default
 until all events are added to the model.
\end_layout

\begin_layout Standard
After adding the last event, 
\emph on
end_add_event()
\emph default
 must be called to inform the model the ending of adding events.
 In C++:
\end_layout

\begin_layout LyX-Code
m.end_add_event();
\end_layout

\begin_layout Standard
In Python:
\end_layout

\begin_layout LyX-Code
m.end_add_event()
\end_layout

\begin_layout Standard
Additional arguments for 
\emph on
end_add_event()
\emph default
 are discussed in the API Reference.
\end_layout

\begin_layout Standard
In addition to this manual, you can get online help from a Python interpreter
 using the 
\family typewriter
help
\family default
 command:
\end_layout

\begin_layout LyX-Code
>>> help(MaxentModel)         # get help for all available functions
\end_layout

\begin_layout LyX-Code
>>> help(MaxentModel.predict) # get help for a specific function
\end_layout

\begin_layout Section
Training the Model
\end_layout

\begin_layout Standard
Train a Maximum Entropy Model is relatively easy.
 Here are some examples:
\end_layout

\begin_layout Standard
For C++ and Python:
\end_layout

\begin_layout LyX-Code
m.train(); // train the model with default training method
\begin_inset Newline newline
\end_inset

m.train(30, "lbfgs"); // train the model with 30 iterations of L-BFGS method
\begin_inset Newline newline
\end_inset

m.train(100, "gis", 2); // train the model with 100 iterations of GIS method
\begin_inset Newline newline
\end_inset

and apply Gaussian Prior smoothing with a global variance of 2
\begin_inset Newline newline
\end_inset

m.train(30, "lbfgs", 2, 1E-03); // set terminate tolerance to 1E-03
\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout Standard
The training methods can be either 
\begin_inset Quotes eld
\end_inset

gis
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

lbfgs
\begin_inset Quotes erd
\end_inset

 (default).
 The Gaussion prior 
\begin_inset Formula $\sigma^{2}$
\end_inset

 is used to regularize the model by seeking a Maximum a Posteriori (MAP)
 solution.
 
\end_layout

\begin_layout Standard
Also, if m.verbose is set to 1 (default is 0) 
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The verbose flag can be turned on by setting maxent.verbose = 1 in C++, and
 using 
\family typewriter
maxent.set_verbose(1)
\family default
 in python.)
\end_layout

\end_inset

, training progress will be printed to stdout.
 So you will see something like this on your screen:
\end_layout

\begin_layout LyX-Code
Total 125997 training events added
\begin_inset Newline newline
\end_inset

Total 0 heldout events added
\begin_inset Newline newline
\end_inset

Reducing events (cutoff is 1)...
\begin_inset Newline newline
\end_inset

Reduced to 65232 training events
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Starting L-BFGS iterations...
\begin_inset Newline newline
\end_inset

Number of Predicates:  5827
\begin_inset Newline newline
\end_inset

Number of Outcomes:    34
\begin_inset Newline newline
\end_inset

Number of Parameters:  8202
\begin_inset Newline newline
\end_inset

Number of Corrections: 5
\begin_inset Newline newline
\end_inset

Tolerance: 1.000000E-05
\begin_inset Newline newline
\end_inset

Gaussian Penalty: on
\begin_inset Newline newline
\end_inset

Optimized version
\begin_inset Newline newline
\end_inset

iter  eval     log-likelihood  training accuracy   heldout accuracy
\begin_inset Newline newline
\end_inset

==================================================================
\begin_inset Newline newline
\end_inset

  0      1-3.526361E+00  0.008%     N/A
\begin_inset Newline newline
\end_inset

  0      1-3.387460E+00  40.380%     N/A
\begin_inset Newline newline
\end_inset

  1      3-2.907289E+00  40.380%     N/A
\begin_inset Newline newline
\end_inset

  2      4-2.266155E+00  44.352%     N/A
\begin_inset Newline newline
\end_inset

  3      5-2.112264E+00  47.233%     N/A
\begin_inset Newline newline
\end_inset

  4      6-1.946646E+00  51.902%     N/A
\begin_inset Newline newline
\end_inset

  5      7-1.832639E+00  52.944%     N/A
\begin_inset Newline newline
\end_inset

  6      8-1.718746E+00  53.109%     N/A
\begin_inset Newline newline
\end_inset

  7      9-1.612014E+00  56.934%     N/A
\begin_inset Newline newline
\end_inset

  8     10-1.467009E+00  62.744%     N/A
\begin_inset Newline newline
\end_inset

  9     11-1.346299E+00  65.729%     N/A
\begin_inset Newline newline
\end_inset

 10     12-1.265980E+00  67.696%     N/A
\begin_inset Newline newline
\end_inset

 11     13-1.203896E+00  69.463%     N/A
\begin_inset Newline newline
\end_inset

 12     14-1.150394E+00  71.434%     N/A
\begin_inset Newline newline
\end_inset

 13     15-1.081878E+00  71.901%     N/A
\begin_inset Newline newline
\end_inset

 14     16-1.069843E+00  70.638%     N/A
\begin_inset Newline newline
\end_inset

 15     17-9.904556E-01  76.113%     N/A
\begin_inset Newline newline
\end_inset

Maximum numbers of 15 iterations reached in 183.195 seconds
\begin_inset Newline newline
\end_inset

Highest log-likelihood: -9.904556E-01
\end_layout

\begin_layout Standard
You can save a trained model to a file and load it back later: In C++ and
 Python:
\end_layout

\begin_layout LyX-Code
m.save("new_model");
\begin_inset Newline newline
\end_inset

m.load("new_model");
\end_layout

\begin_layout Standard
A file named 
\family typewriter
new_model
\family default
 will be created.
 The model contains the definition of context predicates, outcomes, mapping
 between features and feature ids and the optimal parameter weight for each
 feature.
\end_layout

\begin_layout Standard
If the optional parameter 
\emph on
binary
\emph default
 is true and the library is compiled with zlib support, a compressed binary
 model file will be saved which is both faster to load and smaller than
 a plain text model.
 The format of model file will be detected automatically during loading:
 
\end_layout

\begin_layout LyX-Code
m.save("new_model", true); //save a (compressed) binary model
\begin_inset Newline newline
\end_inset

m.load("new_model");       //load it from disk
\end_layout

\begin_layout Section
Using the Model
\end_layout

\begin_layout Standard
The use of the model is straightforward.
 The 
\emph on
eval()
\emph default
 function will return the probability 
\begin_inset Formula $p(y|x)$
\end_inset

 of an outcome 
\begin_inset Formula $y$
\end_inset

 given some context 
\begin_inset Formula $x$
\end_inset

: In C++:
\end_layout

\begin_layout LyX-Code
m.eval(context, outcome);
\end_layout

\begin_layout Standard

\emph on
eval_all()
\emph default
 is useful if we want to get the whole conditional distribution for a given
 context: In C++:
\end_layout

\begin_layout LyX-Code
std::vector<pair<std::string, double> > probs;
\begin_inset Newline newline
\end_inset

m.eval_all(context, probs);
\end_layout

\begin_layout Standard

\emph on
eval_all()
\emph default
 will put the probability distribution into the vector 
\family typewriter
probs
\family default
.
 The items in 
\family typewriter
probs
\family default
 are the outcome labels paired with their corresponding probabilities.
 If the third parameter 
\emph on
sort_result
\emph default
 is true (default) 
\emph on
eval_all()
\emph default
 will automatically sort the output distribution in descendant order: the
 first item will have the highest probability in the distribution.
\end_layout

\begin_layout Standard
The Python binding has two slightly different 
\family typewriter
eval()
\family default
 methods.
 The first is 
\family typewriter
eval()
\family default
 which returns the most probable class label for a given context:
\end_layout

\begin_layout LyX-Code
label = m.eval(context)
\end_layout

\begin_layout LyX-Code

\end_layout

\begin_layout Standard
The second is 
\family typewriter
eval_all()
\family default
 , which returns the whole conditional distributions in a list of (label,
 probability) pairs.
 This equals to the C++ 
\family typewriter
eval()
\family default
 function:
\end_layout

\begin_layout Standard

\family typewriter
result = m.eval_all(context)
\end_layout

\begin_layout Standard
Please consult the API Reference for a more detailed explanation of each
 class and function.
\end_layout

\begin_layout Section
Case Study: Building a maxent Part-of-Speech Tagger
\end_layout

\begin_layout Standard
This section discusses the steps involved in building a Part-of-Speech (POS)
 tagger for English in detail.
 A faithful implementation of the tagger described in 
\begin_inset CommandInset citation
LatexCommand citep
key "ratnaparkhi98maximum"
literal "true"

\end_inset

 will be constructed with this toolkit in Python language.
 When trained on 00-18 sections and tested on 19-24 sections of Wall Street
 Journal corpus, the final tagger achieves an accuracy of more than 96%.
\end_layout

\begin_layout Subsection
The Tagging Model
\end_layout

\begin_layout Standard
The task of POS tag assignment is to assign correct POS tags to a word stream
 (typically a sentence).
 The following table lists a word sequence and its corresponding tags (taken
 form 
\begin_inset CommandInset citation
LatexCommand citep
key "ratnaparkhi98maximum"
literal "true"

\end_inset

):
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="3" columns="8">
<features tabularvalignment="middle">
<column alignment="left" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\emph on
Word:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 the 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 stories 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 about 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 well-heeled 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 communities 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 and 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 developers 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\emph on
Tag:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 DT 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 NNS 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 IN 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 JJ 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 NNS 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 CC 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 NNS 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\emph on
Position:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 1 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 2 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 3 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 4 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 5 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 6 
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 7  
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To attack this problem with the Maximum Entropy Model, we can build a conditiona
l model that calculates the probability of a tag 
\begin_inset Formula $y$
\end_inset

, given some contextual information 
\begin_inset Formula $x$
\end_inset

: 
\begin_inset Formula 
\[
p(y|x)=\frac{1}{Z(x)}\exp\left[\sum_{i=1}^{k}\lambda_{i}f_{i}(x,y)\right]
\]

\end_inset

 Thus the possibility of a tag sequence 
\begin_inset Formula $\{t_{1},t_{2},\dots,t_{n}\}$
\end_inset

 over a sentence 
\begin_inset Formula $\{w_{1},w_{2},\dots,w_{n}\}$
\end_inset

 can be represented as the product of each 
\begin_inset Formula $p(y|x)$
\end_inset

 with the assumption that the probability of each tag 
\begin_inset Formula $y$
\end_inset

 depends only on a limited context information 
\begin_inset Formula $x$
\end_inset

: 
\begin_inset Formula 
\[
p(t_{1},t_{2},\dots,t_{n}|w_{1},w_{2},\dots,w_{n})\approx\prod_{i=1}^{n}p(y_{i}|x_{i})
\]

\end_inset

 Given a sentence 
\begin_inset Formula $\{w_{1},w_{2},\dots,w_{n}\}$
\end_inset

 we can generate the 
\begin_inset Formula $K$
\end_inset

 best tag sequence candidates up to that point in the search and finally
 select the highest candidate as our tagging result.
 
\end_layout

\begin_layout Subsection
Feature Selection
\end_layout

\begin_layout Standard
Following 
\begin_inset CommandInset citation
LatexCommand citep
key "ratnaparkhi98maximum"
literal "true"

\end_inset

, we select features used in the tagging model by applying a set of feature
 templates to the training data.
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="13" columns="2">
<features tabularvalignment="middle">
<column alignment="left" valignment="top" width="0pt">
<column alignment="left" valignment="top" width="0pt">
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Condition 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 Contextual Predicates 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $w_{i}$
\end_inset

 is not rare 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $w_{i}=X$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $w_{i}$
\end_inset

 is rare 
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $X$
\end_inset

 is prefix of 
\begin_inset Formula $w_{i}$
\end_inset

, 
\begin_inset Formula $|X|\leq4$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $X$
\end_inset

 is suffix of 
\begin_inset Formula $w_{i}$
\end_inset

, 
\begin_inset Formula $|X|\leq4$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $X$
\end_inset

 contains number 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $X$
\end_inset

 contains uppercase character 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $X$
\end_inset

 contains hyphen 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\forall w_{i}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $t_{i-1}=X$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $t_{i-w}t_{i-1}=XY$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $w_{i-1}=X$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $w_{i-2}=X$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $w_{i+1}=X$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 
\begin_inset Formula $w_{i+2}=X$
\end_inset

 
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
Please note that if a word is rare (occurs less than 5 times in the training
 set (WSJ corpus 00-18)) several additional contextual predicates are used
 to help predict the tag based on the word's form.
 A useful feature might be: 
\begin_inset Formula 
\[
f(x,y)=\left\{ \begin{array}{ll}
1 & \mbox{if y=VBG and }current\_suffix\_is\_ing(x)=true\\
0 & \mbox{otherwise}
\end{array}\right.
\]

\end_inset

 and is represented as a literal string: 
\begin_inset Quotes eld
\end_inset

suffix=ing_VBG
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
Here is a list of some features gathered form training data (WSJ corpus):
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="16" columns="1">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
curword=years 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tag-1=CD 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
word-2=, 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tag-1,2=,,CD 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
word+1=old 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
curword=old 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
word-1=years 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tag-1=NNS 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
tag-1,2=CD,NNS 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
word+2=will 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
word-1=old 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
prefix=E 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
prefix=El 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
suffix=r 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
suffix=er 
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
suffix=ier  
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Only features occur more than 10 times are preserved.
 Features for rare words are selected with a frequency cutoff of 5.
 
\end_layout

\begin_layout Subsection
Training The Model
\end_layout

\begin_layout Standard
Once the feature set is defined, it is easy to train a maxent tagging model
 with this toolkit.
\end_layout

\begin_layout Standard
First, we need to create a MaxentModel instance and add events to it: 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

from maxent import MaxentModel
\end_layout

\begin_layout Plain Layout

m = MaxentModel()
\end_layout

\begin_layout Plain Layout

m.begin_add_event()
\end_layout

\begin_layout Plain Layout

m.add_event("suffix=ing", "VBG", 1)
\end_layout

\begin_layout Plain Layout

...
\end_layout

\begin_layout Plain Layout

m.end_add_event()
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset

 Next, let's call L-BFGS training routine to train a maxent model with 100
 iterations: 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

m.train(100, "lbfgs")
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout

 
\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

Total 125997 training events added
\end_layout

\begin_layout Plain Layout

Total 0 heldout events added
\end_layout

\begin_layout Plain Layout

Reducing events (cutoff is 1)...
\end_layout

\begin_layout Plain Layout

Reduced to 65232 training events
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Starting L-BFGS iterations...
\end_layout

\begin_layout Plain Layout

Number of Predicates:  5827
\end_layout

\begin_layout Plain Layout

Number of Outcomes:    34
\end_layout

\begin_layout Plain Layout

Number of Parameters:  8202
\end_layout

\begin_layout Plain Layout

Number of Corrections: 5
\end_layout

\begin_layout Plain Layout

Tolerance: 1.000000E-05
\end_layout

\begin_layout Plain Layout

Gaussian Penalty: on
\end_layout

\begin_layout Plain Layout

Optimized version
\end_layout

\begin_layout Plain Layout

iter  eval     log-likelihood  training accuracy   heldout accuracy
\end_layout

\begin_layout Plain Layout

==================================================================
\end_layout

\begin_layout Plain Layout

  0      1-3.526361E+00  0.008%     N/A
\end_layout

\begin_layout Plain Layout

  0      1-3.387460E+00  40.380%     N/A
\end_layout

\begin_layout Plain Layout

  1      3-2.907289E+00  40.380%     N/A
\end_layout

\begin_layout Plain Layout

  2      4-2.266155E+00  44.352%     N/A
\end_layout

\begin_layout Plain Layout

  3      5-2.112264E+00  47.233%     N/A
\end_layout

\begin_layout Plain Layout

  4      6-1.946646E+00  51.902%     N/A
\end_layout

\begin_layout Plain Layout

  5      7-1.832639E+00  52.944%     N/A
\end_layout

\begin_layout Plain Layout

  6      8-1.718746E+00  53.109%     N/A
\end_layout

\begin_layout Plain Layout

  7      9-1.612014E+00  56.934%     N/A
\end_layout

\begin_layout Plain Layout

  8     10-1.467009E+00  62.744%     N/A
\end_layout

\begin_layout Plain Layout

  9     11-1.346299E+00  65.729%     N/A
\end_layout

\begin_layout Plain Layout

 10     12-1.265980E+00  67.696%     N/A
\end_layout

\begin_layout Plain Layout

 11     13-1.203896E+00  69.463%     N/A
\end_layout

\begin_layout Plain Layout

 12     14-1.150394E+00  71.434%     N/A
\end_layout

\begin_layout Plain Layout

 13     15-1.081878E+00  71.901%     N/A
\end_layout

\begin_layout Plain Layout

 14     16-1.069843E+00  70.638%     N/A
\end_layout

\begin_layout Plain Layout

 15     17-9.904556E-01  76.113%     N/A
\end_layout

\begin_layout Plain Layout

Maximum numbers of 15 iterations reached in 183.195 seconds
\end_layout

\begin_layout Plain Layout

Highest log-likelihood: -9.904556E-01
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset

 After training finishes, save the model to a file: 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

m.save("tagger");
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset

 This will create a file called 
\family typewriter
tagger
\family default
 on disk.
 
\end_layout

\begin_layout Subsection
Using The Tagger
\end_layout

\begin_layout Standard
A state-of-the-art POS tagger that faithfully implements the search algorithm
 described in 
\begin_inset CommandInset citation
LatexCommand citep
key "ratnaparkhi98maximum"
literal "true"

\end_inset

, page 43 is included in the toolkit under 
\family typewriter
example/postagger/
\family default
 directory.
\end_layout

\begin_layout Standard
When trained on 00-18 sections of WSJ corpus and tested on 19-24 sections,
 this tagger boasts a word accuracy of 97.31% on known words and 87.39% on
 unknown words with a sentence accuracy of 57.95% and an overall 96.64% word
 accuracy.
\end_layout

\begin_layout Standard
The main executable stript for training a tagger model is 
\family typewriter
postrainer.py
\family default
: 
\end_layout

\begin_layout LyX-Code
usage: postrainer.py [options] model
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

options:
\begin_inset Newline newline
\end_inset

  -h, --help            show this help message and exit
\begin_inset Newline newline
\end_inset

  -fFILE, --file=FILE   train a ME model with data from FILE
\begin_inset Newline newline
\end_inset

  --heldout=FILE        use heldout events from FILE
\begin_inset Newline newline
\end_inset

  --events_out=EVENTS_OUT
\begin_inset Newline newline
\end_inset

                        write training(heldout) events to file
\begin_inset Newline newline
\end_inset

  -mMETHOD, --method=METHOD
\begin_inset Newline newline
\end_inset

                        select training method [lbfgs,gis]
\begin_inset Newline newline
\end_inset

                        [default=lbfgs]
\begin_inset Newline newline
\end_inset

  -cCUTOFF, --cutoff=CUTOFF
\begin_inset Newline newline
\end_inset

                        discard feature with frequency < CUTOFF when training
\begin_inset Newline newline
\end_inset

                        [default=10]
\begin_inset Newline newline
\end_inset

  -rRARE, --rare=RARE   use special feature for rare word with frequency
 < RARE
\begin_inset Newline newline
\end_inset

                        [default=5]
\begin_inset Newline newline
\end_inset

  -gGAUSSIAN, --gaussian=GAUSSIAN
\begin_inset Newline newline
\end_inset

                        apply Gaussian penality when training
\begin_inset Newline newline
\end_inset

                        [default=0.0]
\begin_inset Newline newline
\end_inset

  -b, --binary          save events in binary format for fast loading
\begin_inset Newline newline
\end_inset

                        [default=off]
\begin_inset Newline newline
\end_inset

  --ev_cutoff=EV_CUTOFF
\begin_inset Newline newline
\end_inset

                        discard event with frequency < CUTOFF when training
\begin_inset Newline newline
\end_inset

                        [default=1]
\begin_inset Newline newline
\end_inset

  --iters=ITERS         how many iterations are required for
\begin_inset Newline newline
\end_inset

                        training[default=15]
\begin_inset Newline newline
\end_inset

  --fast                use psyco to speed up training if possible
\begin_inset Newline newline
\end_inset

  -TTYPE, --type=TYPE   choose context type [default for English]
\end_layout

\begin_layout Standard
To train 00-18 sections of WSJ corpus (in file 00_18.sent, one sentence per
 line) with 100 iterations of L-BFGS, Gaussian coefficient 0.8 and save result
 model to 
\begin_inset Quotes eld
\end_inset

wsj
\begin_inset Quotes erd
\end_inset

:
\end_layout

\begin_layout LyX-Code
./postrainer.py -f 00_18.sent --iters 100 -g 0.8 wsj
\end_layout

\begin_layout Standard
The corresponding output during training is sent to stdout:
\end_layout

\begin_layout LyX-Code
First pass: gather word frequency information
\begin_inset Newline newline
\end_inset

1000 lines
\begin_inset Newline newline
\end_inset

2000 lines
\begin_inset Newline newline
\end_inset

3000 lines
\begin_inset Newline newline
\end_inset

4000 lines
\begin_inset Newline newline
\end_inset

.
 .
 .
 
\begin_inset Newline newline
\end_inset

51000 lines
\begin_inset Newline newline
\end_inset

44520 words found in training data
\begin_inset Newline newline
\end_inset

Saving word frequence information to 00_18.sent.wordfreq
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Second pass: gather features and tag dict to be used in tagger
\begin_inset Newline newline
\end_inset

feature cutoff:10
\begin_inset Newline newline
\end_inset

rare word freq:5
\begin_inset Newline newline
\end_inset

1000 lines
\begin_inset Newline newline
\end_inset

2000 lines
\begin_inset Newline newline
\end_inset

3000 lines
\begin_inset Newline newline
\end_inset

4000 lines
\begin_inset Newline newline
\end_inset

.
 .
 .
\begin_inset Newline newline
\end_inset

51000 lines
\begin_inset Newline newline
\end_inset

675386 features found
\begin_inset Newline newline
\end_inset

12092 words found in pos dict
\begin_inset Newline newline
\end_inset

Applying cutoff 10 to features
\begin_inset Newline newline
\end_inset

66519 features remained after cutoff
\begin_inset Newline newline
\end_inset

saving features to file wsj.features
\begin_inset Newline newline
\end_inset

Saving tag dict object to wsj.tagdict done
\begin_inset Newline newline
\end_inset

Third pass:training ME model...
\begin_inset Newline newline
\end_inset

1000 lines
\begin_inset Newline newline
\end_inset

2000 lines
\begin_inset Newline newline
\end_inset

3000 lines
\begin_inset Newline newline
\end_inset

4000 lines
\begin_inset Newline newline
\end_inset

.
 .
 .
\begin_inset Newline newline
\end_inset

51000 lines
\begin_inset Newline newline
\end_inset

Total 969825 training events added
\begin_inset Newline newline
\end_inset

Total 0 heldout events added
\begin_inset Newline newline
\end_inset

Reducing events (cutoff is 1)...
\begin_inset Newline newline
\end_inset

Reduced to 783427 training events
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Starting L-BFGS iterations...
\begin_inset Newline newline
\end_inset

Number of Predicates:  28653
\begin_inset Newline newline
\end_inset

Number of Outcomes:    45
\begin_inset Newline newline
\end_inset

Number of Parameters:  66519
\begin_inset Newline newline
\end_inset

Number of Corrections: 5
\begin_inset Newline newline
\end_inset

Tolerance:             1.000000E-05
\begin_inset Newline newline
\end_inset

Gaussian Penalty:      on
\begin_inset Newline newline
\end_inset

Optimized version
\begin_inset Newline newline
\end_inset

iter  eval     loglikelihood  training accuracy   heldout accuracy
\begin_inset Newline newline
\end_inset

==================================================================
\begin_inset Newline newline
\end_inset

  0      1-3.806662E+00  0.005%     N/A
\begin_inset Newline newline
\end_inset

  0      1-3.636210E+00  47.771%     N/A
\begin_inset Newline newline
\end_inset

  1      3-3.015621E+00  47.771%     N/A
\begin_inset Newline newline
\end_inset

  2      4-2.326449E+00  50.274%     N/A
\begin_inset Newline newline
\end_inset

  3      5-1.750152E+00  56.182%     N/A
\begin_inset Newline newline
\end_inset

  4      6-1.497112E+00  61.177%     N/A
\begin_inset Newline newline
\end_inset

  5      7-1.373379E+00  64.895%     N/A
\begin_inset Newline newline
\end_inset

  .
 .
 .
\begin_inset Newline newline
\end_inset

 94     96-1.990776E-01  97.584%     N/A
\begin_inset Newline newline
\end_inset

 95     97-1.984520E-01  97.602%     N/A
\begin_inset Newline newline
\end_inset

 96     98-1.976996E-01  97.612%     N/A
\begin_inset Newline newline
\end_inset

 97     99-1.968460E-01  97.665%     N/A
\begin_inset Newline newline
\end_inset

 98    100-1.961286E-01  97.675%     N/A
\begin_inset Newline newline
\end_inset

 99    101-1.951691E-01  97.704%     N/A
\begin_inset Newline newline
\end_inset

100    102-1.946537E-01  97.689%     N/A
\begin_inset Newline newline
\end_inset

Maximum numbers of 100 iterations reached in 3817.37 seconds
\begin_inset Newline newline
\end_inset

Highest loglikehood: -1.946537E-01
\begin_inset Newline newline
\end_inset

training finished
\begin_inset Newline newline
\end_inset

saving tagger model to wsj done
\end_layout

\begin_layout Standard
A script 
\emph on
maxent_tagger.py
\emph default
 is provided to tag new sentences using previously trained tagger model:
\end_layout

\begin_layout Standard
To tag new sentences using wsj model:
\end_layout

\begin_layout LyX-Code
usage: maxent_tagger.py [options] -m model file
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

options:
\begin_inset Newline newline
\end_inset

  -h, --help            show this help message and exit
\begin_inset Newline newline
\end_inset

  -oOUTPUT, --output=OUTPUT
\begin_inset Newline newline
\end_inset

                        write tagged result to OUTPUT
\begin_inset Newline newline
\end_inset

  -mMODEL, --model=MODEL
\begin_inset Newline newline
\end_inset

                        load trained model from MODEL
\begin_inset Newline newline
\end_inset

  -t, --test            test mode, include original tag in output
\begin_inset Newline newline
\end_inset

  -v, --verbose         
\begin_inset Newline newline
\end_inset

  -q, --quiet           
\begin_inset Newline newline
\end_inset

  -TTYPE, --type=TYPE   choose context type
\end_layout

\begin_layout Standard
The tagging result will be sent to stdout, one sentence per line.
\end_layout

\begin_layout Chapter
Command Line Utility
\end_layout

\begin_layout Section
The maxent Program
\end_layout

\begin_layout Standard
For convenience, a command line program 
\family typewriter
maxent
\family default
 is provided to carry out some common operations like constructing ME model
 from a data file, predicting labels of unseen data and performing N-fold
 cross validation.
 The source code 
\family typewriter
src/maxent.cpp
\family default
 also demonstrates the use of the C++ interface.
\end_layout

\begin_layout Section
Data Format
\end_layout

\begin_layout Standard
maxent uses a data format similar to other classifiers:
\end_layout

\begin_layout LyX-Code
(BNF-like representation)
\begin_inset Newline newline
\end_inset

<event>   .=.
 <label> <feature>[:<fvalue>] <feature>[:<fvalue>] ...
 
\begin_inset Newline newline
\end_inset

<feature> .=.
 string
\begin_inset Newline newline
\end_inset

<fvalue>  .=.
 float
\begin_inset Newline newline
\end_inset

<label>   .=.
 string
\begin_inset Newline newline
\end_inset

<line>    .=.
 <event>
\end_layout

\begin_layout Standard
Where label and feature are treated as literal 
\begin_inset Quotes eld
\end_inset

string
\begin_inset Quotes erd
\end_inset

s.
 If a feature
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Stickily speaking, this is context predicate.
\end_layout

\end_inset

 is followed with a ':' and a float value (must be non-negative), that number
 is regarded as feature value.
 Otherwise, the feature values are assumed to be 1 (binary feature).
\end_layout

\begin_layout Standard

\series bold
Important:
\series default
 You must either specify all feature values or omit all of them.
 You can not mix them in a data file.
\end_layout

\begin_layout Standard
Here's a sample data file:
\end_layout

\begin_layout LyX-Code
Outdoor Sunny Happy 
\begin_inset Newline newline
\end_inset

Outdoor Sunny Happy Dry 
\begin_inset Newline newline
\end_inset

Outdoor Sunny Happy Humid 
\begin_inset Newline newline
\end_inset

Outdoor Sunny Sad Dry 
\begin_inset Newline newline
\end_inset

Indoor Rainy Happy Humid 
\begin_inset Newline newline
\end_inset

Indoor Rainy Happy Dry 
\begin_inset Newline newline
\end_inset

Indoor Rainy Sad Dry 
\begin_inset Newline newline
\end_inset

.
 .
 .
\end_layout

\begin_layout Standard
Here 
\family typewriter
Outdoor
\family default
 and 
\family typewriter
Indoor
\family default
 are both labels (outcomes) and all other strings are features (contextual
 predicates).
\end_layout

\begin_layout Standard
If numeric features are present, they are treated as feature values (must
 be non-negative).
 This format is compatible with that used by other classifiers such as libsvm
 or svm-light where feature must be real value.
 For example, the following data is taken from a Text Categorization task
 in libsvm file format:
\end_layout

\begin_layout LyX-Code
+1 4:1.0 6:2 9:7 14:1 20:12 25:1 27:0.37 31:1 
\begin_inset Newline newline
\end_inset

+1 4:8 6:91 14:1 20:1 29:1 30:13 31:1 39:1 
\begin_inset Newline newline
\end_inset

+1 6:1 9:7 14:1 20:111 24:1 25:1 28:1 29:0.21
\begin_inset Newline newline
\end_inset

-1 6:6 9:1 14:1 23:1 35:1 39:1 46:1 49:1
\begin_inset Newline newline
\end_inset

-1 6:1 49:1 53:1 55:1 80:1 86:1 102:1
\end_layout

\begin_layout Section
Examples
\end_layout

\begin_layout Standard
Now assuming we have training data in train.txt and testing data in test.txt.
 The following commands illustrate the typical usage of 
\family typewriter
the 
\emph on
maxent
\family default
\emph default
 utility:
\end_layout

\begin_layout Standard
Create an ME model named model1 from train.txt with 30 iterations of L-BFGS
 (default)
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
Cygwin users: due to a bug in Cygwin's implantation of getopt_long(), all
 options passed after training filename is discarded.
 You should specify all options 
\emph on
before
\emph default
 training filename: maxent -m model1 -i 30 train.txt.
\end_layout

\end_inset

:
\end_layout

\begin_layout LyX-Code
maxent train.txt -m model1 -i 30
\end_layout

\begin_layout Standard
If -b flag is present then the model file is saved in binary format, which
 is much faster to load/save than plain text format.
 The format of model file is automatically detected when loading.
 You need not to specify -b to load a binary model.
 If the library is compiled with 
\family typewriter
zlib
\family default
, binary model will be saved in gzip compressed format.
\end_layout

\begin_layout LyX-Code
save a binary model:
\begin_inset Newline newline
\end_inset

maxent train.txt -b -m model1 -i 30
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

then predict new samples with the newly created model:
\begin_inset Newline newline
\end_inset

maxent -p test.txt -m model1
\end_layout

\begin_layout Standard
By default, 
\family typewriter
maxent
\family default
 will try to read data through 
\family typewriter
mmap()
\family default
 system call if available.
 If this causes problems, 
\family typewriter
--nommap
\family default
 option will disable 
\family typewriter
mmap()
\family default
 call and use the slower standard I/O instead.
\end_layout

\begin_layout Standard
Sometimes we only want to know the testing accuracy of a model trained from
 a given training data.
 The train/prediction steps can be combined into a single step without explicitl
y saving/loading the model file:
\end_layout

\begin_layout LyX-Code
maxent train.txt test.txt
\end_layout

\begin_layout Standard
Performing 10-fold cross-validation on train.txt and report accuracy:
\end_layout

\begin_layout LyX-Code
maxent -n 10 train.txt
\end_layout

\begin_layout Standard
When 
\family typewriter
-v
\family default
 option is set, verbose messages will be printed to stdout: 
\end_layout

\begin_layout LyX-Code
maxent train.txt -m model1 -v
\begin_inset Newline newline
\end_inset

Total 180 training events added
\begin_inset Newline newline
\end_inset

Total 0 heldout events added
\begin_inset Newline newline
\end_inset

Reducing events (cutoff is 1)...
\begin_inset Newline newline
\end_inset

Reduced to 177 training events
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Starting L-BFGS iterations...
\begin_inset Newline newline
\end_inset

Number of Predicates:  9757
\begin_inset Newline newline
\end_inset

Number of Outcomes:    2
\begin_inset Newline newline
\end_inset

Number of Parameters:  11883
\begin_inset Newline newline
\end_inset

Number of Corrections: 5
\begin_inset Newline newline
\end_inset

Tolerance:             1.000000E-05
\begin_inset Newline newline
\end_inset

Gaussian Penalty:      off
\begin_inset Newline newline
\end_inset

Optimized version
\begin_inset Newline newline
\end_inset

iter  eval     loglikelihood  training accuracy   heldout accuracy
\begin_inset Newline newline
\end_inset

==================================================================
\begin_inset Newline newline
\end_inset

  0      1-6.931472E-01  38.889%     N/A
\begin_inset Newline newline
\end_inset

  1      2-2.440559E-01  86.111%     N/A
\begin_inset Newline newline
\end_inset

  2      3-1.358731E-01  98.333%     N/A
\begin_inset Newline newline
\end_inset

  3      4-1.058029E-01  98.889%     N/A
\begin_inset Newline newline
\end_inset

  4      5-5.949606E-02  99.444%     N/A
\begin_inset Newline newline
\end_inset

  5      6-3.263124E-02  100.000%     N/A
\begin_inset Newline newline
\end_inset

  6      7-1.506045E-02  100.000%     N/A
\begin_inset Newline newline
\end_inset

  7      8-7.390649E-03  100.000%     N/A
\begin_inset Newline newline
\end_inset

  8      9-3.623262E-03  100.000%     N/A
\begin_inset Newline newline
\end_inset

  9     10-1.661110E-03  100.000%     N/A
\begin_inset Newline newline
\end_inset

 10     11-6.882981E-04  100.000%     N/A
\begin_inset Newline newline
\end_inset

 11     12-4.081801E-04  100.000%     N/A
\begin_inset Newline newline
\end_inset

 12     13-1.907085E-04  100.000%     N/A
\begin_inset Newline newline
\end_inset

 13     14-9.775213E-05  100.000%     N/A
\begin_inset Newline newline
\end_inset

 14     15-4.831358E-05  100.000%     N/A
\begin_inset Newline newline
\end_inset

 15     16-2.423319E-05  100.000%     N/A
\begin_inset Newline newline
\end_inset

 16     17-1.666308E-05  100.000%     N/A
\begin_inset Newline newline
\end_inset

 17     18-5.449101E-06  100.000%     N/A
\begin_inset Newline newline
\end_inset

 18     19-3.448578E-06  100.000%     N/A
\begin_inset Newline newline
\end_inset

 19     20-1.600556E-06  100.000%     N/A
\begin_inset Newline newline
\end_inset

 20     21-8.334602E-07  100.000%     N/A
\begin_inset Newline newline
\end_inset

 21     22-4.137602E-07  100.000%     N/A
\begin_inset Newline newline
\end_inset

Training terminats succesfully in 1.3125 seconds
\begin_inset Newline newline
\end_inset

Highest log-likelihood: -2.068951E-07
\end_layout

\begin_layout Standard
Predict data in test.txt with model1 and save predicated labels (outcome
 label with highest probability) to output.txt, one label per line:
\end_layout

\begin_layout LyX-Code
maxent -p -m model1 -o output.txt test.txt
\end_layout

\begin_layout Standard
If the detail flag is given in prediction mode, full distribution will
 be outputted:
\end_layout

\begin_layout LyX-Code
<outcome1> <prob1> <outcome2> <prob2> ...
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

maxent -p -m model1 --detail -o output.txt test.txt 
\end_layout

\begin_layout Standard
It is possible to specify a set of 
\emph on
heldout
\emph default
 data to monitor the performance of model in each iteration of training:
 the decline of accuracy on heldout data may indicate some 
\emph on
overfitting
\emph default
.
\end_layout

\begin_layout LyX-Code
maxent -m model1 train.txt --heldout heldout.txt -v
\begin_inset Newline newline
\end_inset

Loading training events from train.txt
\begin_inset Newline newline
\end_inset

.
\begin_inset Newline newline
\end_inset

Loading heldout events from heldout.txt
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Total 1000 training events added
\begin_inset Newline newline
\end_inset

Total 99 heldout events added
\begin_inset Newline newline
\end_inset

Reducing events (cutoff is 1)...
\begin_inset Newline newline
\end_inset

Reduced to 985 training events
\begin_inset Newline newline
\end_inset

Reduced to 99 heldout events
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Starting L-BFGS iterations...
\begin_inset Newline newline
\end_inset

Number of Predicates:  24999
\begin_inset Newline newline
\end_inset

Number of Outcomes:    2
\begin_inset Newline newline
\end_inset

Number of Parameters:  30304
\begin_inset Newline newline
\end_inset

Number of Corrections: 5
\begin_inset Newline newline
\end_inset

Tolerance:             1.000000E-05
\begin_inset Newline newline
\end_inset

Gaussian Penalty:      off
\begin_inset Newline newline
\end_inset

Optimized version
\begin_inset Newline newline
\end_inset

iter  eval     loglikelihood  training accuracy   heldout accuracy
\begin_inset Newline newline
\end_inset

==================================================================
\begin_inset Newline newline
\end_inset

  0      1-6.931472E-01  43.300%     48.485%
\begin_inset Newline newline
\end_inset

  1      2-3.821936E-01  74.400%     71.717%
\begin_inset Newline newline
\end_inset

  2      3-1.723962E-01  95.600%     95.960%
\begin_inset Newline newline
\end_inset

  3      4-1.465401E-01  97.100%     97.980%
\begin_inset Newline newline
\end_inset

  4      5-1.196789E-01  97.600%     97.980%
\begin_inset Newline newline
\end_inset

  5      6-9.371452E-02  97.800%     97.980%
\begin_inset Newline newline
\end_inset

  6      7-6.035709E-02  98.700%     97.980%
\begin_inset Newline newline
\end_inset

  7      8-3.297382E-02  99.700%     98.990%
\begin_inset Newline newline
\end_inset

  8      9-1.777857E-02  99.800%     98.990%
\begin_inset Newline newline
\end_inset

  9     10-9.939370E-03  99.900%     100.000%
\begin_inset Newline newline
\end_inset

  9     10-8.610207E-02  95.900%     94.949%
\begin_inset Newline newline
\end_inset

 10     12-8.881104E-03  99.900%     98.990%
\begin_inset Newline newline
\end_inset

 11     13-4.874563E-03  99.900%     98.990%
\begin_inset Newline newline
\end_inset

 12     14-2.780725E-03  99.900%     98.990%
\begin_inset Newline newline
\end_inset

 13     15-1.139578E-03  100.000%     98.990%
\begin_inset Newline newline
\end_inset

 14     16-5.539811E-04  100.000%     98.990%
\begin_inset Newline newline
\end_inset

 15     17-2.344039E-04  100.000%     98.990%
\begin_inset Newline newline
\end_inset

 16     18-1.371225E-04  100.000%     98.990%
\begin_inset Newline newline
\end_inset

Training terminats succesfully in 8.5625 seconds
\begin_inset Newline newline
\end_inset

Highest log-likelihood: -9.583916E-08
\end_layout

\begin_layout Standard
In this example, it seems performance peaks at iteration 9.
 Further training actually brings down the accuracy on the heldout data,
 although the training accuracy continues to increase.
 Applying a Gaussian prior can help avoid overfitting, just use 
\family typewriter
-g float
\family default
 to specify the global Gaussian variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
Finally, 
\family typewriter
-h
\family default
 option will bring up a short help screen: 
\end_layout

\begin_layout LyX-Code
maxent -h
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Purpose:
\begin_inset Newline newline
\end_inset

  A command line utility to train (test) a maxent model from a file.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Usage: maxent [OPTIONS]...
 [FILES]...
\begin_inset Newline newline
\end_inset

   -h         --help            Print help and exit
\begin_inset Newline newline
\end_inset

   -V         --version         Print version and exit
\begin_inset Newline newline
\end_inset

   -v         --verbose         verbose mode (default=off)
\begin_inset Newline newline
\end_inset

   -mSTRING   --model=STRING    set model filename
\begin_inset Newline newline
\end_inset

   -b         --binary          save model in binary format (default=off)
\begin_inset Newline newline
\end_inset

   -oSTRING   --output=STRING   prediction output filename
\begin_inset Newline newline
\end_inset

              --detail          output full distribution in prediction mode
 (default=off)
\begin_inset Newline newline
\end_inset

   -iINT      --iter=INT        iterations for training algorithm (default='30')
\begin_inset Newline newline
\end_inset

   -gFLOAT    --gaussian=FLOAT  set Gaussian prior, disable if 0 (default='0.0')
\begin_inset Newline newline
\end_inset

   -cINT      --cutoff=INT      set event cutoff (default='1')
\begin_inset Newline newline
\end_inset

              --heldout=STRING  specify heldout data for training
\begin_inset Newline newline
\end_inset

   -r         --random          randomizing data in cross validation (default=of
f)
\begin_inset Newline newline
\end_inset

              --nommap          do not use mmap() to read data (slow) (default=o
ff)
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

   Group: MODE
\begin_inset Newline newline
\end_inset

   -p         --predict         prediction mode, default is training mode
\begin_inset Newline newline
\end_inset

   -nINT      --cv=INT          N-fold cross-validation mode (default='0')
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

   Group: Parameter Estimate Method
\begin_inset Newline newline
\end_inset

              --lbfgs           use L-BFGS parameter estimation (default)
\begin_inset Newline newline
\end_inset

              --gis             use GIS parameter estimation
\end_layout

\begin_layout Chapter
API Reference
\end_layout

\begin_layout Standard
\begin_inset CommandInset label
LatexCommand label
name "chap:api"

\end_inset


\end_layout

\begin_layout Section
C++ API
\end_layout

\begin_layout Standard
This section is generated automatically from C++ source code.
 Unfortunately, sometimes the generated interfaces are overly complex to
 be useful.
 I hope the Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:tutorial"

\end_inset

 is enough for most people.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
input{namespacemaxent}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
input{classmaxent_1_1MaxentModel}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
input{classmaxent_1_1Trainer}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Python API
\end_layout

\begin_layout Standard
This section is under construction
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
dots
\end_layout

\end_inset

.
\end_layout

\begin_layout Chapter
Acknowledgment
\end_layout

\begin_layout Standard
The author owns his thanks to: 
\end_layout

\begin_layout Itemize
Developers of 
\begin_inset Flex URL
status collapsed

\begin_layout Plain Layout

http://maxent.sourceforge.net
\end_layout

\end_inset

, the java implementation of MaxEnt with GIS training algorithm.
 Actually, this toolkit evolves from an early attempt to port Java maxent
 to C++.
 
\end_layout

\begin_layout Itemize
Robert Malouf.
 Dr.
 Malouf kindly answered my questions on maxent and provides his excellent
 implementation on four maxent parameter estimation algorithms.
 
\end_layout

\begin_layout Itemize
Jorge Nocedal, for the excellent Fortran L-BFGS implementation.
\end_layout

\begin_layout Itemize
And of course all users who provided valuable feedback and bug reports.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "maxent"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document
